# [1] í•„ìš”í•œ íŒ¨í‚¤ì§€ import
import streamlit as st
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import os
import glob
import requests
import langdetect
import docx2txt
import PyPDF2

# [2] ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (ë¬¸ì„œ ingestion ì‹œ ì‚¬ìš©ë¨)
def extract_text_from_file(file_path):
    if file_path.endswith(".txt"):
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    elif file_path.endswith(".docx"):
        return docx2txt.process(file_path)
    elif file_path.endswith(".pdf"):
        text = ""
        with open(file_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                text += page.extract_text() + "\n"
        return text
    return ""

# [3] ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¡œë”©í•˜ê³ , íŒŒì¼ëª… ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë°˜í™˜
def load_text_files_with_metadata(lang_dir):
    chunks, metadatas = [], []
    for file_path in glob.glob(os.path.join(lang_dir, "*")):
        text = extract_text_from_file(file_path)
        for para in text.split("\n\n"):
            if para.strip():
                chunks.append(para.strip())
                metadatas.append({"filename": os.path.basename(file_path)})
    return chunks, metadatas

# [4] í…ìŠ¤íŠ¸ ì²­í¬ â†’ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
def embed_chunks(chunks, model):
    return model.encode(chunks, convert_to_numpy=True)

# [5] ë²¡í„° DB ìƒì„± ë° ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• (FAISS ì‚¬ìš©)
def build_faiss_index(embeddings):
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index

# [6] ì§ˆë¬¸ ë²¡í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìœ ì‚¬í•œ ì²­í¬ 3ê°œ ê²€ìƒ‰
def search_similar_chunks(question, model, index, chunks, metadatas, top_k=3):
    q_emb = model.encode([question], convert_to_numpy=True)
    _, indices = index.search(q_emb, top_k)
    return [chunks[i] for i in indices[0]], [metadatas[i] for i in indices[0]]

# [7] LLMì— ë„£ì„ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì²­í¬ + ì§ˆë¬¸ í¬í•¨)
def build_prompt(top_chunks, question, lang):
    system_prompt = {
        "en": "You are an internal chatbot for AIG Connect. Answer the question based on the context below.",
        "ko": "ë‹¹ì‹ ì€ AIG Connectì˜ ë‚´ë¶€ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.",
        "ja": "ã‚ãªãŸã¯AIG Connectã®ç¤¾å†…ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æ–‡è„ˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
    }
    context = "\n".join(top_chunks)
    prompt = f"""{system_prompt.get(lang, system_prompt['en'])}

Context:
{context}

Question: {question}
Answer:"""
    return prompt

# [8] LLM ì„œë²„ì— í”„ë¡¬í”„íŠ¸ë¥¼ ì „ì†¡í•˜ì—¬ ì‘ë‹µ ë°›ê¸° (Ollama API ë“±)
def call_llm(prompt):
    url = "http://192.168.0.23:11434/api/generate"
    payload = {
        "model": "mistral",
        "prompt": prompt,
        "stream": False
    }
    try:
        res = requests.post(url, json=payload)
        return res.json().get("response", "[No response received]")
    except Exception as e:
        return f"[ERROR] {e}"

# [9] Streamlit í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="AIG Connect Chatbot")
st.title("ğŸ¤– AIG Connect Internal Chatbot (Multilingual)")

# [10] ì–¸ì–´ë³„ë¡œ ë¬¸ì„œ ë¡œë”© ë° ì„ë² ë”©/ì¸ë±ìŠ¤ ìƒì„± â†’ ìºì‹œ ì²˜ë¦¬
@st.cache_resource(show_spinner=False)
def setup_chatbot_by_lang(lang_code):
    folder = f"./docs/{lang_code}"
    if not os.path.exists(folder):
        return [], [], None, None
    chunks, metadatas = load_text_files_with_metadata(folder)
    model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = embed_chunks(chunks, model)
    index = build_faiss_index(embeddings)
    return chunks, metadatas, model, index

# [11] ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì´ì „ ì§ˆë¬¸/ë‹µë³€ ê¸°ë¡ ì €ì¥ìš©)
if "history" not in st.session_state:
    st.session_state.history = []

# [12] ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ UI â†’ ì—¬ê¸°ì„œë¶€í„° ì‚¬ìš©ì ì•¡ì…˜ìœ¼ë¡œ íë¦„ ì‹œì‘ë¨
question = st.text_input("Enter your question below:")

# [13] ì§ˆë¬¸ì´ ì œì¶œë˜ì—ˆì„ ë•Œ ì‹¤í–‰ë˜ëŠ” íë¦„
if st.button("Send") and question.strip():
    # [13-1] ì–¸ì–´ ê°ì§€
    lang = langdetect.detect(question)
    st.markdown(f"ğŸŒ Detected language: `{lang}`")
    
    # [13-2] ì–¸ì–´ë³„ ë¬¸ì„œ ì„ë² ë”©/ë²¡í„° DB êµ¬ì„± ë¶ˆëŸ¬ì˜¤ê¸°
    chunks, metadatas, model, index = setup_chatbot_by_lang(lang)
    
    if not model:
        st.error(f"No documents found for language: {lang}")
    else:
        # [13-3] ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰
        top_chunks, top_metas = search_similar_chunks(question, model, index, chunks, metadatas)
        # [13-4] í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = build_prompt(top_chunks, question, lang)
        # [13-5] LLM í˜¸ì¶œ â†’ ì‘ë‹µ ìƒì„±
        answer = call_llm(prompt)
        # [13-6] ê²°ê³¼ ì„¸ì…˜ì— ì €ì¥
        st.session_state.history.append((question, answer, top_metas))

# [14] ì´ì „ Q&A ì´ë ¥ í™”ë©´ì— ì¶œë ¥
if st.session_state.history:
    st.markdown("---")
    for i, (q, a, metas) in enumerate(reversed(st.session_state.history), 1):
        st.markdown(f"**ğŸ§‘ You:** {q}")
        st.markdown(f"**ğŸ¤– Bot:** {a}")
        sources = sorted(set(m['filename'] for m in metas))
        st.markdown("ğŸ“ **Sources:** " + ", ".join(sources))
        st.download_button("ğŸ“¥ Download answer", a, file_name=f"response_{i}.txt", use_container_width=True)
        st.markdown("---")
