# chatbot_web_streamlit.py (ì–¸ì–´ ê°ì§€ í›„ ê²½ê³  ë° UI ê°œì„ )

import streamlit as st
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import os
import glob
import requests
import langdetect
import docx2txt
import PyPDF2

# ì–¸ì–´ë³„ UI í…ìŠ¤íŠ¸
UI_TEXT = {
    "en": {
        "title": "Insurance Chatbot",
        "input": "Enter your question below:",
        "warning": "The question seems to be in '{detected}', but the selected language is '{selected}'. Please switch the language or rephrase your question.",
        "send": "Send"
    },
    "ko": {
        "title": "ë³´í—˜ ì±—ë´‡",
        "input": "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
        "warning": "ì§ˆë¬¸ì´ '{detected}' ì–¸ì–´ë¡œ ì‘ì„±ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì„ íƒëœ ì–¸ì–´ëŠ” '{selected}'ì…ë‹ˆë‹¤. ì–¸ì–´ë¥¼ ì „í™˜í•˜ê±°ë‚˜ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì‘ì„±í•´ ì£¼ì„¸ìš”.",
        "send": "ì „ì†¡"
    },
    "ja": {
        "title": "ä¿é™ºãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
        "input": "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š",
        "warning": "è³ªå•ã¯ã€Œ{detected}ã€ã®è¨€èªã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚é¸æŠã•ã‚ŒãŸè¨€èªã¯ã€Œ{selected}ã€ã§ã™ã€‚è¨€èªã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã‹ã€è³ªå•ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚",
        "send": "é€ä¿¡"
    },
    "zh-cn": {
        "title": "ä¿é™©èŠå¤©æœºå™¨äºº",
        "input": "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š",
        "warning": "æ£€æµ‹åˆ°çš„é—®é¢˜è¯­è¨€ä¸ºâ€œ{detected}â€ï¼Œä½†å½“å‰é€‰æ‹©çš„è¯­è¨€æ˜¯â€œ{selected}â€ã€‚è¯·æ›´æ¢è¯­è¨€æˆ–é‡æ–°è¾“å…¥é—®é¢˜ã€‚",
        "send": "å‘é€"
    }
}

# 1. ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ

def extract_text_from_file(file_path):
    if file_path.endswith(".txt"):
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    elif file_path.endswith(".docx"):
        return docx2txt.process(file_path)
    elif file_path.endswith(".pdf"):
        text = ""
        with open(file_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                text += page.extract_text() + "\n"
        return text
    return ""

# 2. ë¬¸ì„œ ë¡œë”© (ì–¸ì–´ë³„)

def load_text_files_with_metadata(lang_dir):
    chunks, metadatas = [], []
    for file_path in glob.glob(os.path.join(lang_dir, "*")):
        text = extract_text_from_file(file_path)
        for para in text.split("\n\n"):
            if para.strip():
                chunks.append(para.strip())
                metadatas.append({"filename": os.path.basename(file_path)})
    return chunks, metadatas

# 3. ì„ë² ë”©

def embed_chunks(chunks, model):
    return model.encode(chunks, convert_to_numpy=True)

# 4. ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•

def build_faiss_index(embeddings):
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index

# 5. ì§ˆë¬¸ ê¸°ë°˜ ê²€ìƒ‰

def search_similar_chunks(question, model, index, chunks, metadatas, top_k=3):
    q_emb = model.encode([question], convert_to_numpy=True)
    _, indices = index.search(q_emb, top_k)
    return [chunks[i] for i in indices[0]], [metadatas[i] for i in indices[0]]

# 6. í”„ë¡¬í”„íŠ¸ ìƒì„±

def build_prompt(top_chunks, question, lang):
    system_prompt = {
        "en": "Answer the question based on the context below.",
        "ko": "ì•„ë˜ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.",
        "ja": "ä»¥ä¸‹ã®æ–‡è„ˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚",
        "zh-cn": "è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ã€‚"
    }
    context = "\n".join(top_chunks)
    prompt = f"""{system_prompt.get(lang, system_prompt['en'])}

Context:
{context}

Question: {question}
Answer:"""
    return prompt

# 7. LLM ì„œë²„ í˜¸ì¶œ

def call_llm(prompt):
    url = "http://192.168.0.23:11434/api/generate"
    payload = {
        "model": "mistral",
        "prompt": prompt,
        "stream": False
    }
    try:
        res = requests.post(url, json=payload)
        return res.json().get("response", "[No response received]")
    except Exception as e:
        return f"[ERROR] {e}"

# 8. Streamlit UI

st.set_page_config(page_title="Insurance Chatbot", layout="wide")

# ì–¸ì–´ ì„ íƒ (ìƒë‹¨ ì˜¤ë¥¸ìª½ ìœ„ì¹˜)
st.sidebar.title("ğŸŒ Language")
selected_lang = st.sidebar.radio("", ["en", "ko", "ja", "zh-cn"], horizontal=True)

ui = UI_TEXT[selected_lang]
st.title(f"ğŸ¤– {ui['title']}")

if "history" not in st.session_state:
    st.session_state.history = []

# 9. ì•± ì‹¤í–‰ ì‹œì ì— ì–¸ì–´ë³„ ë¬¸ì„œ ë¡œë”©/ì„ë² ë”©/ì¸ë±ì‹±ì„ ë¯¸ë¦¬ ìˆ˜í–‰

@st.cache_resource(show_spinner=False)
def initialize_chatbots():
    chatbot_dict = {}
    for lang_code in ["en", "ko", "ja", "zh-cn"]:
        folder = f"./docs/{lang_code}"
        if os.path.exists(folder):
            chunks, metadatas = load_text_files_with_metadata(folder)
            model = SentenceTransformer("all-MiniLM-L6-v2")
            embeddings = embed_chunks(chunks, model)
            index = build_faiss_index(embeddings)
            chatbot_dict[lang_code] = (chunks, metadatas, model, index)
    return chatbot_dict

chatbots = initialize_chatbots()

# 10. ì§ˆë¬¸ ì…ë ¥ ë° ì‘ë‹µ ì²˜ë¦¬

question = st.text_input(ui["input"])

if st.button(ui["send"]) and question.strip():
    detected_lang = langdetect.detect(question)
    if detected_lang != selected_lang:
        warning_text = ui["warning"].format(detected=detected_lang, selected=selected_lang)
        st.warning(warning_text)
    elif selected_lang in chatbots:
        chunks, metadatas, model, index = chatbots[selected_lang]
        top_chunks, top_metas = search_similar_chunks(question, model, index, chunks, metadatas)
        prompt = build_prompt(top_chunks, question, selected_lang)
        answer = call_llm(prompt)
        st.session_state.history.append((question, answer))
    else:
        st.error(f"No documents found for language: {selected_lang}")

# 11. ê²°ê³¼ ì¶œë ¥

if st.session_state.history:
    st.markdown("---")
    for i, (q, a) in enumerate(reversed(st.session_state.history), 1):
        st.markdown(f"**ğŸ§‘ You:** {q}")
        st.markdown(f"**ğŸ¤– Bot:** {a}")
        st.download_button("ğŸ“¥ Download answer", a, file_name=f"response_{i}.txt", use_container_width=True)
        st.markdown("---")
